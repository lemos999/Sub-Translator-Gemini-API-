# filename: app.py

# ---------------------------------------------------------
# [1. ì˜ì¡´ì„± ì„í¬íŠ¸]
# í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì—¬ê¸°ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤.
# ---------------------------------------------------------
import streamlit as st                 # ì›¹ UI í”„ë ˆì„ì›Œí¬. í™”ë©´ì„ ê·¸ë¦¬ê³  ì‚¬ìš©ì ì…ë ¥ì„ ë°›ëŠ”ë‹¤.
import google.generativeai as genai    # êµ¬ê¸€ Gemini AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ SDK.
from google.generativeai.types import HarmCategory, HarmBlockThreshold # AIì˜ ìœ í•´ì„± ì½˜í…ì¸  í•„í„°ë§ ì„¤ì •ì„ ì œì–´í•˜ê¸° ìœ„í•¨.
import re                              # ì •ê·œ í‘œí˜„ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬. SRT íŒŒì¼ì˜ ë³µì¡í•œ í…ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.
import time                            # ì‹œê°„ ê´€ë ¨ í•¨ìˆ˜. API í˜¸ì¶œ ì‚¬ì´ì— ë”œë ˆì´ë¥¼ ì£¼ê±°ë‚˜, ì‘ì—… ì†Œìš” ì‹œê°„ì„ ì¸¡ì •í•œë‹¤.
import chardet                         # íŒŒì¼ì˜ ë¬¸ì ì¸ì½”ë”©(ì˜ˆ: UTF-8, CP949)ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬. í•œê¸€ ê¹¨ì§ ë°©ì§€ì— í•„ìˆ˜.
import json                            # JSON ë°ì´í„° í˜•ì‹ì„ ë‹¤ë£¨ê¸° ìœ„í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬. AIì™€ì˜ í†µì‹  í”„ë¡œí† ì½œì— ì‚¬ìš©ëœë‹¤.
import datetime                        # ë‚ ì§œì™€ ì‹œê°„ í˜•ì‹ì„ ë‹¤ë£¨ê¸° ìœ„í•¨. ETA(ì˜ˆìƒ ì¢…ë£Œ ì‹œê°„) í‘œì‹œì— ì‚¬ìš©ëœë‹¤.


# ---------------------------------------------------------
# [2. í•µì‹¬ ë¡œì§ í•¨ìˆ˜]
# ìë§‰ ë²ˆì—­ ì‘ì—…ì˜ ì‹¤ì œ ë‘ë‡Œì™€ ì†ë°œì´ ë˜ëŠ” í•¨ìˆ˜ë“¤.
# ---------------------------------------------------------

def detect_encoding(file_byte):
    """
    íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€í•œë‹¤.
    ì‚¬ìš©ìê°€ ì–´ë–¤ í˜•ì‹ì˜ SRT íŒŒì¼ì„ ì˜¬ë¦¬ë“  (Windowsì—ì„œ ë§Œë“  CP949, Mac/Linuxì˜ UTF-8 ë“±)
    í•œê¸€ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡ ë°©ì§€í•˜ëŠ” ì¤‘ìš”í•œ í•¨ìˆ˜.
    
    Args:
        file_byte (bytes): íŒŒì¼ì˜ ì›ë³¸ ë°”ì´íŠ¸ ë°ì´í„°.
    
    Returns:
        str: ê°ì§€ëœ ì¸ì½”ë”© ì´ë¦„ (ì˜ˆ: 'utf-8').
    """
    # chardet ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„.
    result = chardet.detect(file_byte)
    # ë¶„ì„ ê²°ê³¼ì—ì„œ 'encoding' í‚¤ ê°’ë§Œ ë°˜í™˜.
    return result['encoding']

def parse_srt(content):
    """
    SRT íŒŒì¼ì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„,
    [ì¸ë±ìŠ¤, íƒ€ì„ì½”ë“œ, í…ìŠ¤íŠ¸] êµ¬ì¡°ì˜ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í•´(íŒŒì‹±)í•œë‹¤.
    ì´ í•¨ìˆ˜ ë•ë¶„ì— ë©”íƒ€ë°ì´í„°(ì‹œê°„)ì™€ ë°ì´í„°(í…ìŠ¤íŠ¸)ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤.
    
    Args:
        content (str): SRT íŒŒì¼ì˜ ì „ì²´ ë¬¸ìì—´ ë‚´ìš©.
    
    Returns:
        list: ê° ìë§‰ ë¸”ë¡ì´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ëœ ë¦¬ìŠ¤íŠ¸.
    """
    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ SRTì˜ ë°˜ë³µì ì¸ êµ¬ì¡°ë¥¼ ì°¾ì•„ë‚¸ë‹¤.
    # (\d+): ìë§‰ ì¸ë±ìŠ¤ (ìˆ«ì 1ê°œ ì´ìƒ)
    # \s*\n: ê³µë°±(0ê°œ ì´ìƒ) í›„ ì¤„ë°”ê¿ˆ
    # (\d{2}:\d{2}:\d{2},\d{3}\s*-->\s*\d{2}:\d{2}:\d{2},\d{3}): 'ì‹œ:ë¶„:ì´ˆ,ë°€ë¦¬ì´ˆ --> ì‹œ:ë¶„:ì´ˆ,ë°€ë¦¬ì´ˆ' í˜•ì‹ì˜ íƒ€ì„ì½”ë“œ
    # \s*\n: ê³µë°± í›„ ì¤„ë°”ê¿ˆ
    # ((?:.|\n)*?): ìë§‰ í…ìŠ¤íŠ¸. ëª¨ë“  ë¬¸ì(.) ë˜ëŠ” ì¤„ë°”ê¿ˆ(\n)ì´ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë©°, ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ ìˆë‹¤.
    # (?=\n\d+\s*\n|\Z): ë‹¤ìŒ ìë§‰ ë¸”ë¡(ì¤„ë°”ê¿ˆ+ìˆ«ì+ì¤„ë°”ê¿ˆ)ì´ ë‚˜ì˜¤ê¸° ì§ì „ê¹Œì§€ ë˜ëŠ” íŒŒì¼ì˜ ë(\Z)ê¹Œì§€ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë³¸ë‹¤.
    pattern = re.compile(r'(\d+)\s*\n(\d{2}:\d{2}:\d{2},\d{3}\s*-->\s*\d{2}:\d{2}:\d{2},\d{3})\s*\n((?:.|\n)*?)(?=\n\d+\s*\n|\Z)', re.MULTILINE)
    # ì •ê·œ í‘œí˜„ì‹ì— ë§ëŠ” ëª¨ë“  ë¶€ë¶„ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    matches = pattern.findall(content)
    
    parsed_data = []
    # ì°¾ì€ ê° ë¸”ë¡ì„ ìˆœíšŒí•˜ë©° ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ê°€ê³µ.
    for match in matches:
        parsed_data.append({
            'index': match[0],         # ì²« ë²ˆì§¸ ê·¸ë£¹: ì¸ë±ìŠ¤
            'time': match[1],          # ë‘ ë²ˆì§¸ ê·¸ë£¹: íƒ€ì„ì½”ë“œ
            'text': match[2].strip()   # ì„¸ ë²ˆì§¸ ê·¸ë£¹: í…ìŠ¤íŠ¸ (ì•ë’¤ ê³µë°± ì œê±°)
        })
    return parsed_data

def chunk_text(parsed_data, chunk_size=1500):
    """
    íŒŒì‹±ëœ ìë§‰ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, APIê°€ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”
    ì¼ì •í•œ í¬ê¸°(chunk_size)ì˜ ë¬¶ìŒ(ì²­í¬)ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
    
    Args:
        parsed_data (list): parse_srt í•¨ìˆ˜ê°€ ë°˜í™˜í•œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸.
        chunk_size (int): í•˜ë‚˜ì˜ ì²­í¬ì— í¬í•¨ë  ìµœëŒ€ ê¸€ì ìˆ˜.
    
    Returns:
        list: ë”•ì…”ë„ˆë¦¬ë“¤ì´ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì¸ 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ (ì²­í¬ì˜ ë¦¬ìŠ¤íŠ¸).
    """
    chunks = []
    current_chunk = []
    current_length = 0
    
    # ëª¨ë“  ìë§‰ ë°ì´í„°ë¥¼ ìˆœíšŒ.
    for item in parsed_data:
        text_len = len(item['text'])
        # í˜„ì¬ ì²­í¬ì— ì´ë²ˆ í…ìŠ¤íŠ¸ë¥¼ ë”í•˜ë©´ ìµœëŒ€ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸.
        if current_length + text_len > chunk_size:
            # ì´ˆê³¼í•˜ë©´, ì§€ê¸ˆê¹Œì§€ì˜ ì²­í¬ë¥¼ ìµœì¢… ì²­í¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€.
            chunks.append(current_chunk)
            # í˜„ì¬ ì²­í¬ì™€ ê¸¸ì´ë¥¼ ì´ˆê¸°í™”.
            current_chunk = []
            current_length = 0
        
        # í˜„ì¬ ì²­í¬ì— ìë§‰ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³ , ê¸¸ì´ë¥¼ ëˆ„ì .
        current_chunk.append(item)
        current_length += text_len
        
    # ë§ˆì§€ë§‰ì— ë‚¨ì€ ì²­í¬ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë„ ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€.
    if current_chunk:
        chunks.append(current_chunk)
        
    return chunks

def clean_json_text(text):
    """
    AIê°€ ë°˜í™˜í•œ í…ìŠ¤íŠ¸ì—ì„œ ìˆœìˆ˜í•œ JSON ë¶€ë¶„ë§Œ ì™¸ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤.
    AIê°€ "ë²ˆì—­ ê²°ê³¼ì…ë‹ˆë‹¤: { ... }" ì™€ ê°™ì´ ë¶ˆí•„ìš”í•œ ì‚¬ì¡±ì„ ë¶™ì´ê±°ë‚˜,
    ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json ... ```)ìœ¼ë¡œ ê°ì‹¸ëŠ” ê²½ìš°ì— ëŒ€í•œ ë°©ì–´ ë¡œì§.
    
    Args:
        text (str): AIê°€ ë°˜í™˜í•œ ì›ë³¸ ì‘ë‹µ ë¬¸ìì—´.
        
    Returns:
        str: JSON ê°ì²´ë¡œ ì¶”ì •ë˜ëŠ” ë¶€ë¶„ë§Œ ë‚¨ê¸´ ë¬¸ìì—´.
    """
    try:
        # 1. ê°€ì¥ ë°”ê¹¥ìª½ì˜ '{' ì™€ '}'ë¥¼ ì°¾ì•„ ê·¸ ì‚¬ì´ì˜ ëª¨ë“  ê²ƒì„ ì¶”ì¶œ. ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•.
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        if start_idx != -1 and end_idx != -1:
            json_str = text[start_idx : end_idx + 1]
            return json_str
        
        # 2. ìœ„ ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´, ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì„ ì œê±°í•˜ëŠ” ì˜ˆì „ ë°©ì‹ ì‹œë„.
        text = text.strip()
        if text.startswith("```"):
            text = text.split("\n", 1)[-1]
            text = text.rsplit("\n", 1)[0]
        return text.strip()
    except:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ, ê·¸ëƒ¥ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜ (í›„ì† ë¡œì§ì—ì„œ ì—ëŸ¬ ì²˜ë¦¬).
        return text

def analyze_context(model, full_text, src_lang, tgt_lang):
    """
    ì „ì²´ ìë§‰ì˜ ì¼ë¶€ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ AIì—ê²Œ ë³´ë‚´ê³ ,
    ì‘í’ˆì˜ ì¥ë¥´, í†¤, ì¸ë¬¼ ê´€ê³„, í•µì‹¬ ìš©ì–´ ë“±ì„ ë¶„ì„í•œ 'ì»¨í…ìŠ¤íŠ¸ ê°€ì´ë“œ'ë¥¼ ë°›ì•„ì˜¨ë‹¤.
    
    Args:
        model: ì‚¬ìš©í•  Gemini AI ëª¨ë¸ ê°ì²´.
        full_text (str): ìë§‰ ì „ì²´ í…ìŠ¤íŠ¸.
        src_lang (str): ì†ŒìŠ¤ ì–¸ì–´.
        tgt_lang (str): íƒ€ê²Ÿ ì–¸ì–´.
    
    Returns:
        str: AIê°€ ì‘ì„±í•œ ì»¨í…ìŠ¤íŠ¸ ê°€ì´ë“œ í…ìŠ¤íŠ¸.
    """
    # í† í° ë¹„ìš©ê³¼ ì‹œê°„ì„ ì•„ë¼ê¸° ìœ„í•´ ì „ì²´ê°€ ì•„ë‹Œ ì¼ë¶€ë§Œ ìƒ˜í”Œë§.
    # ê°€ì¥ ì •ë³´ê°€ ë°€ì§‘ëœ ì´ˆë°˜ 3000ìì™€, ë¶„ìœ„ê¸° íŒŒì•…ì„ ìœ„í•œ ì¤‘ê°„ 2000ìë¥¼ ì¡°í•©.
    sample_text = full_text[:3000]
    if len(full_text) > 5000:
        mid = len(full_text) // 2
        sample_text += "\n...\n" + full_text[mid:mid+2000]
    
    # AIì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê³ , ë¬´ì—‡ì„ ë¶„ì„í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•í•˜ê²Œ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸.
    # íŠ¹íˆ "ì¼ë°˜ ë‹¨ì–´ëŠ” ì œì™¸í•˜ë¼"ê³  ëª…ì‹œí•˜ì—¬ ê²°ê³¼ë¬¼ì˜ í’ˆì§ˆì„ ë†’ì„.
    prompt = f"""
    Analyze the following subtitle sample to prepare for translation from {src_lang} to {tgt_lang}.
    
    [Subtitle Sample]
    {sample_text}
    
    [Task]
    Provide a "System Context Guide" for the translator AI.
    
    [Requirements]
    1. **Genre & Tone**: Define the atmosphere.
    2. **Character & Relationships**: Identify key characters. Who speaks formally/informally to whom?
    3. **Consistency Rules (Glossary)**:
       - List ONLY technical terms, proper nouns, or ambiguous words that need consistency.
       - Do NOT list common words (e.g., "farmer", "school") unless they have a special hidden meaning.
       - Keep it minimal and strictly relevant.
    
    [Output Format]
    Write a concise guide in {tgt_lang}.
    """
    
    response = model.generate_content(prompt)
    return response.text.strip()

def translate_chunk(model, text_list, src_lang, tgt_lang, context_guide="", enable_reasoning=False):
    """
    í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë°›ì•„ AIì—ê²Œ ë²ˆì—­ì„ ìš”ì²­í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜.
    ID ì•µì»¤ë§, JSON ê°•ì œ ëª¨ë“œ, ìë™ ì¬ì‹œë„, ì»¨í…ìŠ¤íŠ¸ ì£¼ì…, ì¶”ë¡  ëª¨ë“œ ë“± ëª¨ë“  í•µì‹¬ ê¸°ìˆ ì´ ì§‘ì•½ë˜ì–´ ìˆë‹¤.
    
    Args:
        model: ì‚¬ìš©í•  Gemini AI ëª¨ë¸ ê°ì²´.
        text_list (list): ë²ˆì—­í•  ë¬¸ìì—´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸.
        src_lang (str): ì†ŒìŠ¤ ì–¸ì–´.
        tgt_lang (str): íƒ€ê²Ÿ ì–¸ì–´.
        context_guide (str, optional): ì‚¬ì „ ë¶„ì„ëœ ì»¨í…ìŠ¤íŠ¸ ê°€ì´ë“œ.
        enable_reasoning (bool, optional): ì¶”ë¡  ë²„í‚·(ê³ í’ˆì§ˆ) ëª¨ë“œ í™œì„±í™” ì—¬ë¶€.
    
    Returns:
        tuple: (ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸, ë””ë²„ê·¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬)
    """
    start_time = time.time()  # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´ ì‹œì‘ ì‹œê°„ ê¸°ë¡.
    max_retries = 1           # ë„¤íŠ¸ì›Œí¬ ë“± ì¼ì‹œì  ì˜¤ë¥˜ì— ëŒ€ë¹„í•œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜.
    
    # ë””ë²„ê¹… ë° ìƒíƒœ ë¡œê¹…ì„ ìœ„í•œ ì •ë³´ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬.
    debug_info = {
        "input_json": "", "raw_response": "", "status": "Unknown", "attempts": 0,
        "duration": 0.0, "context_used": "None", "reasoning_mode": "ON" if enable_reasoning else "OFF"
    }

    # [ID ì•µì»¤ë§] ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ [{"id": 0, "text": "..."}, ...] êµ¬ì¡°ë¡œ ë³€í™˜.
    indexed_input = [{"id": i, "text": t} for i, t in enumerate(text_list)]
    input_wrapper = {"items": indexed_input}
    input_json = json.dumps(input_wrapper, ensure_ascii=False)
    debug_info["input_json"] = input_json # ë””ë²„ê¹…ì„ ìœ„í•´ ë³´ë‚¸ JSON ì›ë³¸ ì €ì¥.
    
    # [ì¶”ë¡  ë²„í‚·] ì¶”ë¡  ëª¨ë“œê°€ ì¼œì¡Œì„ ê²½ìš°, í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ì§€ì‹œ ì‚¬í•­ ì‚½ì….
    reasoning_instruction = ""
    if enable_reasoning:
        reasoning_instruction = """
        [MAX REASONING MODE: ON]
        1. Before translating, DEEPLY ANALYZE the nuances, context, and speaker's intent for every line.
        2. Consider the flow of the conversation step-by-step.
        3. Prioritize naturalness and emotional accuracy over literal translation.
        4. YOU MUST OUTPUT ONLY THE JSON.
        """
    
    # [ì»¨í…ìŠ¤íŠ¸ ì£¼ì…] ì»¨í…ìŠ¤íŠ¸ ê°€ì´ë“œê°€ ìˆì„ ê²½ìš°, í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€.
    context_section = ""
    if context_guide:
        debug_info["context_used"] = context_guide
        context_section = f"""
        [CONTEXT & STYLE GUIDE]
        (You must follow these rules strictly)
        {context_guide}
        --------------------------------------------------
        """
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½.
    prompt = f"""
    You are a professional subtitle translator.
    Translate the "text" field in the JSON objects from {src_lang} to {tgt_lang}.
    {reasoning_instruction}
    {context_section}
    [INPUT JSON]
    {input_json}
    [CRITICAL RULES]
    1. Output MUST be a valid JSON object with a key "translated_items".
    2. "translated_items" is a list of objects: {{"id": integer, "text": "translated_string"}}.
    3. You MUST preserve the "id" exactly as is.
    4. Do NOT merge or split lines. One ID = One Line.
    [OUTPUT SCHEMA]
    {{ "translated_items": [ {{"id": 0, "text": "..."}}, ... ] }}
    """
    
    # AIì˜ ìœ í•´ì„± ì½˜í…ì¸  í•„í„°ë¥¼ ëª¨ë‘ ë¹„í™œì„±í™”. (ì†Œì„¤/ë“œë¼ë§ˆì˜ í­ë ¥ì ì´ê±°ë‚˜ ì„ ì •ì ì¸ ëŒ€ì‚¬ ì°¨ë‹¨ ë°©ì§€)
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
    
    # AI ëª¨ë¸ì˜ ë™ì‘ì„ ì œì–´í•˜ëŠ” ì„¤ì •.
    generation_config = {
        "temperature": 0.2 if enable_reasoning else 0.1, # ì¶”ë¡  ëª¨ë“œì¼ ë•Œ ì°½ì˜ì„±ì„ ì•½ê°„ ë†’ì—¬ ë” ë‚˜ì€ í‘œí˜„ì„ ì°¾ë„ë¡ í•¨.
        "response_mime_type": "application/json"        # [JSON ê°•ì œ ëª¨ë“œ] APIê°€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ë„ë¡ ê°•ì œ.
    }
    
    # [ìë™ ì¬ì‹œë„ ë¡œì§]
    for attempt in range(max_retries + 1):
        debug_info["attempts"] = attempt + 1
        try:
            # AIì—ê²Œ ë²ˆì—­ ìš”ì²­.
            response = model.generate_content(prompt, safety_settings=safety_settings, generation_config=generation_config)
            
            raw_text = response.text
            debug_info["raw_response"] = raw_text # ë°›ì€ ì‘ë‹µ ì›ë³¸ ì €ì¥.
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ìˆœìˆ˜ JSONë§Œ ì¶”ì¶œ.
            cleaned_text = clean_json_text(raw_text)
            result_json = json.loads(cleaned_text)
            
            # ì•½ì†ëœ í‚¤("translated_items")ë¡œ ë°ì´í„° ì¶”ì¶œ.
            if "translated_items" in result_json:
                items = result_json["translated_items"]
            else: # í˜¹ì‹œ AIê°€ ë‹¤ë¥¸ í‚¤ë¥¼ ì‚¬ìš©í–ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œ.
                items = list(result_json.values())[0]
            
            # AIê°€ ì‘ë‹µ ìˆœì„œë¥¼ ë’¤ì„ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬.
            items.sort(key=lambda x: x.get("id", -1))
            
            # IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ ìˆœì„œì— ë§ê²Œ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ìµœì¢… ë¦¬ìŠ¤íŠ¸ ìƒì„±.
            translated_list = []
            for i in range(len(text_list)):
                found = False
                for item in items:
                    if item.get("id") == i:
                        translated_list.append(item.get("text", ""))
                        found = True
                        break
                if not found: # ë§Œì•½ AIê°€ íŠ¹ì • IDë¥¼ ëˆ„ë½í–ˆë‹¤ë©´, ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´ (ì‹±í¬ ê¹¨ì§ ë°©ì§€).
                    translated_list.append(text_list[i]) 
                    
            # ìµœì¢…ì ìœ¼ë¡œ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê°œìˆ˜ê°€ ê°™ì€ì§€ ê²€ì¦.
            if len(translated_list) != len(text_list):
                raise ValueError(f"Mismatch (In: {len(text_list)}, Out: {len(translated_list)})")
            
            # ëª¨ë“  ê³¼ì •ì´ ì„±ê³µí–ˆì„ ê²½ìš°.
            debug_info["status"] = "Success"
            debug_info["duration"] = round(time.time() - start_time, 2)
            return translated_list, debug_info # ì„±ê³µ ê²°ê³¼ ë°˜í™˜.
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë””ë²„ê·¸ ì •ë³´ ê¸°ë¡.
            debug_info["status"] = f"Error: {str(e)}"
            if attempt < max_retries:
                time.sleep(1) # ì¬ì‹œë„ ì „ 1ì´ˆ ëŒ€ê¸°.
                continue      # ë£¨í”„ì˜ ë‹¤ìŒ ì‹œë„ë¡œ ë„˜ì–´ê°.
            else:
                # ìµœì¢… ì‹¤íŒ¨ ì‹œ, ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì™€ ë””ë²„ê·¸ ì •ë³´ ë°˜í™˜.
                debug_info["duration"] = round(time.time() - start_time, 2)
                return text_list, debug_info

def rebuild_srt(original_data, chunks_translated):
    """
    ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì™€ ì›ë³¸ íŒŒì‹± ë°ì´í„°ë¥¼ ë°›ì•„,
    í•˜ë‚˜ì˜ ì™„ì „í•œ SRT íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ ì¬ì¡°ë¦½í•œë‹¤.
    
    Args:
        original_data (list): parse_srtê°€ ìƒì„±í•œ ì›ë³¸ êµ¬ì¡° ë¦¬ìŠ¤íŠ¸.
        chunks_translated (list): ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë“¤ë¡œ êµ¬ì„±ëœ 2ì°¨ì› ë¦¬ìŠ¤íŠ¸.
    
    Returns:
        str: ìµœì¢… SRT íŒŒì¼ ë‚´ìš© ë¬¸ìì—´.
    """
    # 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¥¼ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ í‰íƒ„í™”.
    flat_translations = [t for chunk in chunks_translated for t in chunk]
    
    output = []
    # ì›ë³¸ ë°ì´í„°ì™€ ë²ˆì—­ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´, ë” ì ì€ ìª½ì— ë§ì¶° ì•ˆì „í•˜ê²Œ ìˆœíšŒ.
    limit = min(len(original_data), len(flat_translations))
    for i in range(limit):
        origin = original_data[i]
        trans = flat_translations[i]
        # ì›ë³¸ì˜ [ì¸ë±ìŠ¤, íƒ€ì„ì½”ë“œ]ì™€ ë²ˆì—­ëœ [í…ìŠ¤íŠ¸]ë¥¼ í•©ì³ SRT ë¸”ë¡ ìƒì„±.
        block = f"{origin['index']}\n{origin['time']}\n{trans}\n"
        output.append(block)
    
    # ëª¨ë“  ë¸”ë¡ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì³ ìµœì¢… íŒŒì¼ ë‚´ìš© ìƒì„±.
    return "\n".join(output)

def render_grid(states):
    """
    ì²­í¬ë“¤ì˜ ìƒíƒœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ì‹œê°ì ì¸ ìƒíƒœ ê·¸ë¦¬ë“œ HTMLì„ ìƒì„±í•œë‹¤.
    
    Args:
        states (list): ê° ì²­í¬ì˜ ìƒíƒœ({'status': '...', 'duration': ...})ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸.
    
    Returns:
        str: ê·¸ë¦¬ë“œë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•œ HTML/CSS ì½”ë“œ.
    """
    # CSS ìŠ¤íƒ€ì¼ ì •ì˜: ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒ, ê° ìƒíƒœë³„ ìƒ‰ìƒ, í˜¸ë²„ íš¨ê³¼, ì• ë‹ˆë©”ì´ì…˜ ë“±.
    html = """
    <style>
        .grid-container { display: flex; flex-wrap: wrap; gap: 4px; margin-bottom: 20px; }
        .grid-item { width: 12px; height: 12px; border-radius: 2px; transition: all 0.3s ease; position: relative; }
        .grid-item:hover { transform: scale(1.5); z-index: 10; cursor: help; border: 1px solid #fff; }
        .status-WAITING { background-color: #e5e7eb; }
        .status-RUNNING { background-color: #3b82f6; box-shadow: 0 0 5px #3b82f6; animation: pulse 1s infinite; }
        .status-SUCCESS { background-color: #22c55e; }
        .status-ERROR { background-color: #ef4444; }
        @keyframes pulse { 0% { opacity: 1; } 50% { opacity: 0.5; } 100% { opacity: 1; } }
    </style>
    <div class="grid-container">
    """
    # ê° ì²­í¬ ìƒíƒœì— ë”°ë¼ CSS í´ë˜ìŠ¤ë¥¼ ì ìš©í•˜ê³ , ë§ˆìš°ìŠ¤ë¥¼ ì˜¬ë¦¬ë©´ ìƒì„¸ ì •ë³´ê°€ ë³´ì´ë„ë¡ íˆ´íŒ(title ì†ì„±) ì¶”ê°€.
    for i, state in enumerate(states):
        status = state['status']
        duration = state.get('duration', 0)
        tooltip = f"Chunk {i+1}: {status} ({duration}s)"
        html += f'<div class="grid-item status-{status}" title="{tooltip}"></div>'
    html += "</div>"
    return html


# ---------------------------------------------------------
# [3. UI ë Œë”ë§ ë° ë©”ì¸ ë¡œì§]
# Streamlitì„ ì‚¬ìš©í•˜ì—¬ í™”ë©´ì„ êµ¬ì„±í•˜ê³ , ì‚¬ìš©ì ì…ë ¥ì— ë”°ë¼ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œë‹¤.
# ---------------------------------------------------------

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •: ì œëª©, ë ˆì´ì•„ì›ƒ ë“±.
st.set_page_config(page_title="Ray's Subtitle Translator", layout="wide")

# [ì„¸ì…˜ ìƒíƒœ(Session State) ì´ˆê¸°í™”]
# Streamlitì€ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ìƒí˜¸ì‘ìš©í•  ë•Œë§ˆë‹¤ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰í•˜ëŠ”ë°,
# ì´ ë•Œ ë³€ìˆ˜ë“¤ì´ ë‚ ì•„ê°€ì§€ ì•Šë„ë¡ ë°ì´í„°ë¥¼ ë³´ê´€í•˜ëŠ” 'ë©”ëª¨ë¦¬' ì—­í• ì„ í•œë‹¤.
if "chunks" not in st.session_state: st.session_state["chunks"] = []
if "results" not in st.session_state: st.session_state["results"] = []
if "debugs" not in st.session_state: st.session_state["debugs"] = []
if "parsed_srt" not in st.session_state: st.session_state["parsed_srt"] = []
if "chunk_states" not in st.session_state: st.session_state["chunk_states"] = []
if "is_running" not in st.session_state: st.session_state["is_running"] = False
if "context_guide" not in st.session_state: st.session_state["context_guide"] = ""
if "final_srt_content" not in st.session_state: st.session_state["final_srt_content"] = "" # ìµœì¢… ë²ˆì—­ ê²°ê³¼ë¬¼ì„ ë³´ê´€í•  ê¸ˆê³ .

# [CSS ì£¼ì…]
# Streamlitì˜ ê¸°ë³¸ ìŠ¤íƒ€ì¼ì„ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬, ë” ë¯¸ë ¤í•œ UIë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ CSS.
st.markdown("""
<style>
    [data-testid="stSidebar"] { min-width: 350px; max-width: 500px; }
    .stButton>button { width: 100%; border-radius: 8px; font-weight: bold; }
    .metric-box { border: 1px solid #ddd; padding: 10px; border-radius: 8px; text-align: center; background: #fdfdfd; }
    .metric-val { font-size: 1.5em; font-weight: bold; color: #333; }
    .metric-label { font-size: 0.9em; color: #666; }
    .github-link { text-decoration: none; color: #fafafa; }
    .github-icon svg { width: 20px; height: 20px; fill: currentColor; margin-right: 8px; vertical-align: middle; transition: color 0.2s; }
    .github-link:hover { color: #3b82f6; }
    .footer { font-size: 0.8em; color: #aaa; text-align: center; }
</style>
""", unsafe_allow_html=True) # unsafe_allow_html=TrueëŠ” HTML/CSSë¥¼ ì§ì ‘ ë Œë”ë§í•˜ê¸° ìœ„í•´ í•„ìˆ˜.

# [ì‚¬ì´ë“œë°” UI êµ¬ì„±]
with st.sidebar:
    # ê¹ƒí—ˆë¸Œ ë§í¬ (SVG ì•„ì´ì½˜ í¬í•¨)
    st.markdown("""
    <a href="https://github.com/lemos999" target="_blank" class="github-link">
        <span class="github-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.07-.55-.17-.55-.38 0-.19.01-.82.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21-.15.46-.55.38A8.013 8.013 0 0 1 0 8c0-4.42 3.58-8 8-8Z"></path></svg>
        </span>
        lemos999's GitHub
    </a>
    """, unsafe_allow_html=True)
    
    st.header("âš™ï¸ Settings")
    api_key = st.text_input("Google API Key", type="password")
    
    # API í‚¤ê°€ ì…ë ¥ë˜ë©´, ì¦‰ì‹œ SDK ì„¤ì •.
    if api_key:
        try: genai.configure(api_key=api_key)
        except Exception: pass

    # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ë²„íŠ¼.
    if st.button("ğŸ” Check Models"):
        if not api_key: st.error("API Key Required")
        else:
            try:
                # 'generateContent'ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ í•„í„°ë§í•˜ì—¬ ê°€ì ¸ì˜´.
                models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
                st.session_state["fetched_models"] = models
                st.success(f"Found {len(models)} models!")
            except Exception as e:
                st.error(f"Error: {e}")
    
    selected_model = None
    if "fetched_models" in st.session_state and st.session_state["fetched_models"]:
        # ë¦¬ìŠ¤íŠ¸ì— ëª¨ë¸ì´ ìˆìœ¼ë©´, ë“œë¡­ë‹¤ìš´ ë©”ë‰´ë¥¼ ë³´ì—¬ì¤Œ.
        index = 0
        for i, m in enumerate(st.session_state["fetched_models"]):
            if "flash" in m or "pro" in m: # 'flash'ë‚˜ 'pro' ëª¨ë¸ì„ ê¸°ë³¸ ì„ íƒìœ¼ë¡œ ìœ ë„.
                index = i
                break
        selected_model = st.selectbox("Select Model", st.session_state["fetched_models"], index=index,
                                      format_func=lambda x: x.replace("models/", "")) # UIì—ëŠ” 'models/' ì ‘ë‘ì‚¬ ë¹¼ê³  ë³´ì—¬ì¤Œ.
    else:
        st.selectbox("Select Model", [], disabled=True) # ëª¨ë¸ ì—†ìœ¼ë©´ ë¹„í™œì„±í™”.

    st.divider()
    chunk_size = st.slider("Chunk Size", 500, 15000, 1500, 100) # ì²­í¬ í¬ê¸° ì¡°ì ˆ ìŠ¬ë¼ì´ë”.
    
    st.subheader("ğŸ§  Intelligence")
    enable_reasoning = st.toggle("Enable Reasoning Bucket (Max)", value=False) # ì¶”ë¡  ëª¨ë“œ í† ê¸€.
    
    st.divider()
    col1, col2 = st.columns(2) # ì–¸ì–´ ì„ íƒì„ ì¢Œìš°ë¡œ ë°°ì¹˜í•˜ê¸° ìœ„í•œ ì»¬ëŸ¼.
    with col1: src_lang = st.selectbox("From", ["English", "Korean", "Japanese"])
    with col2: tgt_lang = st.selectbox("To", ["Korean", "English", "Japanese"])
        
    st.divider()
    # [ê¸´ê¸‰ ì •ì§€] is_running ìƒíƒœì¼ ë•Œë§Œ ë²„íŠ¼ì´ ë³´ì„.
    if st.session_state["is_running"]:
        if st.button("ğŸš¨ STOP PROCESS", type="primary"):
            st.session_state["is_running"] = False # í”Œë˜ê·¸ë¥¼ Falseë¡œ ë°”ê¿”ì„œ ë©”ì¸ ë£¨í”„ê°€ ë©ˆì¶”ë„ë¡ í•¨.
            st.warning("Stopping process... Please wait.")
            
    # ì œì‘ì í¬ë ˆë”§.
    st.markdown('<div class="footer">Made by fewweekslater</div>', unsafe_allow_html=True)

# [ë©”ì¸ í˜ì´ì§€ UI êµ¬ì„±]
st.title("ğŸ¬ Subtitle Translator (Context-Aware)")

uploaded_file = st.file_uploader("Upload Subtitle (.srt)", type=["srt"])

# ëª¨ë“  ì¡°ê±´(íŒŒì¼, APIí‚¤, ëª¨ë¸)ì´ ì¶©ì¡±ë˜ì—ˆì„ ë•Œë§Œ ì•„ë˜ ë¡œì§ ì‹¤í–‰.
if uploaded_file and api_key and selected_model:
    bytes_data = uploaded_file.getvalue()
    encoding = detect_encoding(bytes_data) or 'utf-8' # ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨ ì‹œ utf-8ë¡œ ê°•ì œ.
    
    try: content = bytes_data.decode(encoding)
    except: content = bytes_data.decode('utf-8', errors='ignore') # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ì†ìƒëœ ë¬¸ìëŠ” ë¬´ì‹œí•˜ê³  ë””ì½”ë”©.
    
    st.info(f"File loaded. Encoding: {encoding}")
    
    st.divider()
    st.subheader("ğŸ•µï¸ Step 1: Context Analysis (Optional)")
    
    col_a1, col_a2 = st.columns([1, 4])
    with col_a1:
        if st.button("ğŸ§  Analyze Context"):
            with st.spinner("Analyzing..."):
                model = genai.GenerativeModel(selected_model)
                analysis_result = analyze_context(model, content, src_lang, tgt_lang)
                st.session_state["context_guide"] = analysis_result
                st.success("Analysis Done!")
    
    with col_a2:
        # ì‚¬ìš©ìê°€ AIì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆë„ë¡ text_area ì œê³µ.
        context_guide = st.text_area("Context Guide (Edit if needed):", value=st.session_state["context_guide"], height=150)
        st.session_state["context_guide"] = context_guide # ìˆ˜ì •ëœ ë‚´ìš©ì„ ì„¸ì…˜ ìƒíƒœì— ì¦‰ì‹œ ë°˜ì˜.

    st.divider()
    st.subheader("ğŸš€ Step 2: Start Translation")
    
    # [ë²ˆì—­ ì‹œì‘ ë²„íŠ¼]
    if st.button("Start Translation Process", type="primary"):
        parsed = parse_srt(content)
        if not parsed: st.error("Parsing Failed.")
        else:
            st.session_state["is_running"] = True # ì‘ì—… ì‹œì‘ í”Œë˜ê·¸ ON.
            st.session_state["parsed_srt"] = parsed
            st.session_state["chunks"] = chunk_text(parsed, chunk_size=chunk_size)
            
            total = len(st.session_state["chunks"])
            # ëª¨ë“  ìƒíƒœ ë¦¬ìŠ¤íŠ¸ë“¤ì„ ì²­í¬ ê°œìˆ˜ì— ë§ê²Œ ì´ˆê¸°í™”.
            st.session_state["results"] = [None] * total
            st.session_state["debugs"] = [None] * total
            st.session_state["chunk_states"] = [{'status': 'WAITING', 'duration': 0} for _ in range(total)]
            
            # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë¹ˆ ê³µê°„(placeholder) í™•ë³´.
            timer_ph = st.empty()
            grid_ph = st.empty()
            status_ph = st.empty()
            
            model = genai.GenerativeModel(selected_model)
            start_global = time.time()
            final_context = st.session_state["context_guide"]
            
            st.subheader("ğŸ“ Live Execution Log")
            live_log_container = st.container()

            # [ë©”ì¸ ë²ˆì—­ ë£¨í”„]
            for i, chunk in enumerate(st.session_state["chunks"]):
                # ê¸´ê¸‰ ì •ì§€ ë²„íŠ¼ì´ ëˆŒë ¸ëŠ”ì§€ ë§¤ë²ˆ í™•ì¸.
                if not st.session_state["is_running"]:
                    status_ph.warning(f"Stopped at Chunk {i}.")
                    break
                
                # ëŒ€ì‹œë³´ë“œ UI ì—…ë°ì´íŠ¸ (ì²˜ë¦¬ì¤‘ ìƒíƒœ).
                st.session_state["chunk_states"][i]['status'] = 'RUNNING'
                grid_ph.markdown(render_grid(st.session_state["chunk_states"]), unsafe_allow_html=True)
                status_ph.info(f"âš¡ Processing Chunk {i+1}/{total}...")
                
                # íƒ€ì´ë¨¸/ETA ê³„ì‚° ë° í‘œì‹œ.
                elapsed = time.time() - start_global
                avg_time = elapsed / (i if i > 0 else 1)
                eta = avg_time * (total - i)
                timer_ph.markdown(f"""
                <div style="display: flex; gap: 10px; margin-bottom: 20px;">
                    <div class="metric-box" style="flex:1"><div class="metric-label">Elapsed</div><div class="metric-val">{datetime.timedelta(seconds=int(elapsed))}</div></div>
                    <div class="metric-box" style="flex:1"><div class="metric-label">Chunk Avg</div><div class="metric-val">{avg_time:.1f}s</div></div>
                    <div class="metric-box" style="flex:1"><div class="metric-label">ETA</div><div class="metric-val">{datetime.timedelta(seconds=int(eta))}</div></div>
                </div>
                """, unsafe_allow_html=True)
                
                texts = [item['text'] for item in chunk]
                # í•µì‹¬ ë²ˆì—­ í•¨ìˆ˜ í˜¸ì¶œ.
                res, debug = translate_chunk(model, texts, src_lang, tgt_lang, context_guide=final_context, enable_reasoning=enable_reasoning)
                
                # ê²°ê³¼ ì €ì¥.
                st.session_state["results"][i] = res
                st.session_state["debugs"][i] = debug
                
                # ëŒ€ì‹œë³´ë“œ UI ì—…ë°ì´íŠ¸ (ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ).
                is_success = debug['status'] == "Success"
                st.session_state["chunk_states"][i]['status'] = 'SUCCESS' if is_success else 'ERROR'
                st.session_state["chunk_states"][i]['duration'] = debug.get('duration', 0)
                
                # ... ë¼ì´ë¸Œ ë¡œê·¸ UI ì—…ë°ì´íŠ¸ ìƒëµ (ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ì–´ì ¸ì„œ) ...
                
                grid_ph.markdown(render_grid(st.session_state["chunk_states"]), unsafe_allow_html=True)
                time.sleep(0.5) # API ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•½ê°„ì˜ ë”œë ˆì´.
            
            # [ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥]
            # ë£¨í”„ê°€ ëë‚˜ë©´, í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ë¡œ ìµœì¢… SRT ë¬¸ìì—´ì„ ë§Œë“¤ì–´ 'ê¸ˆê³ 'ì— ì €ì¥.
            safe_results = []
            for i, res in enumerate(st.session_state["results"]):
                if res is None: safe_results.append([item['text'] for item in st.session_state["chunks"][i]])
                else: safe_results.append(res)
            st.session_state["final_srt_content"] = rebuild_srt(st.session_state["parsed_srt"], safe_results)

            st.session_state["is_running"] = False # ì‘ì—… ì¢…ë£Œ í”Œë˜ê·¸ OFF.
            status_ph.success("Complete!")
            time.sleep(1)
            st.rerun() # UIë¥¼ 'ì²˜ë¦¬ì¤‘' í™”ë©´ì—ì„œ 'ê²°ê³¼' í™”ë©´ìœ¼ë¡œ ì „í™˜í•˜ê¸° ìœ„í•´ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰.

# [ê²°ê³¼ ë° ìˆ˜ë¦¬(Repair) í™”ë©´]
# ìµœì¢… ê²°ê³¼ë¬¼ì´ 'ê¸ˆê³ 'ì— ì €ì¥ë˜ì–´ ìˆì„ ë•Œë§Œ ì´ ë¶€ë¶„ì„ ê·¸ë¦¼.
if st.session_state.get("final_srt_content"):
    st.divider()
    
    st.subheader("ğŸ“Š Execution Overview")
    st.markdown(render_grid(st.session_state["chunk_states"]), unsafe_allow_html=True)
    
    col_l, col_r = st.columns(2)
    with col_l:
        st.text_area("Original", content, height=400)
    with col_r:
        # í™”ë©´ì— í‘œì‹œë˜ëŠ” í…ìŠ¤íŠ¸ëŠ” 'ê¸ˆê³ 'ì—ì„œ ì§ì ‘ êº¼ë‚´ì˜´.
        st.text_area("Translated", st.session_state["final_srt_content"], height=400)
        
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë„ 'ê¸ˆê³ 'ì— ì €ì¥ëœ ìµœì¢… ê²°ê³¼ë¬¼ì„ ë°ì´í„°ë¡œ ì‚¬ìš©.
    st.download_button(
        "Download Result (.srt)",
        st.session_state["final_srt_content"].encode('utf-8'),
        f"translated_{uploaded_file.name}",
        "text/plain",
        type="primary"
    )
    
    st.divider()
    st.subheader("ğŸ› ï¸ Chunk Inspector")
    
    # [ìˆ˜ë™ ì¬ì‹œë„ ë¡œì§]
    for i, debug in enumerate(st.session_state["debugs"]):
        if debug is None: continue
        is_success = debug['status'] == "Success"
        icon = "âœ…" if is_success else "âŒ"
        duration = debug.get('duration', 0)
        
        default_expanded = not is_success # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í¼ì³ì„œ ë³´ì—¬ì¤Œ.
        with st.expander(f"{icon} Chunk {i+1} ({duration}s)", expanded=default_expanded):
            c1, c2, c3 = st.columns([1, 4, 4])
            with c1:
                # ê° ë²„íŠ¼ì— ê³ ìœ í•œ keyë¥¼ ë¶€ì—¬í•˜ì—¬ ì„œë¡œ êµ¬ë¶„.
                if st.button(f"ğŸ”„ Retry #{i+1}", key=f"retry_{i}"):
                    # st.spinner: 'ì²˜ë¦¬ì¤‘'ì´ë¼ëŠ” ì‹œê°ì  í”¼ë“œë°±ì„ ì£¼ê¸° ìœ„í•¨.
                    with st.spinner(f"Retrying Chunk {i+1}..."):
                        model = genai.GenerativeModel(selected_model)
                        chunk = st.session_state["chunks"][i]
                        texts = [item['text'] for item in chunk]
                        
                        # ì¬ì‹œë„ ì‹¤í–‰.
                        res, new_debug = translate_chunk(model, texts, src_lang, tgt_lang, context_guide=st.session_state["context_guide"], enable_reasoning=enable_reasoning)
                        
                        # ìƒˆë¡œìš´ ê²°ê³¼ë¡œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸.
                        st.session_state["results"][i] = res
                        st.session_state["debugs"][i] = new_debug
                        st.session_state["chunk_states"][i]['status'] = 'SUCCESS' if new_debug['status']=="Success" else 'ERROR'
                        st.session_state["chunk_states"][i]['duration'] = new_debug.get('duration', 0)
                        
                        # ì¬ì‹œë„ í›„ì—ë„ 'ê¸ˆê³ 'ë¥¼ ìµœì‹  ìƒíƒœë¡œ ì—…ë°ì´íŠ¸.
                        safe_results = []
                        for j, r in enumerate(st.session_state["results"]):
                            if r is None: safe_results.append([item['text'] for item in st.session_state["chunks"][j]])
                            else: safe_results.append(r)
                        st.session_state["final_srt_content"] = rebuild_srt(st.session_state["parsed_srt"], safe_results)

                        # ëª¨ë“  ì—…ë°ì´íŠ¸ê°€ ëë‚˜ê³  ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ UI ìƒˆë¡œê³ ì¹¨.
                        st.rerun()

            with c2:
                st.caption("Sent JSON")
                st.code(debug['input_json'], language='json')
            with c3:
                it1, it2 = st.tabs(["Response JSON", "Info"])
                with it1: st.code(debug['raw_response'], language='json')
                with it2:
                     st.caption(f"Reasoning Mode: {debug.get('reasoning_mode', 'OFF')}")
                     st.text_area("Used Context", debug.get('context_used', ''), height=100, disabled=True, key=f"repair_context_{i}")

elif not api_key:
    st.info("ğŸ‘ˆ API Key Required.")